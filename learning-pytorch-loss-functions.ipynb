{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0e1bed46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Learning various pytorch loss function APIs via a video blog\n",
    "Author: Kaikai Zhao\n",
    "Reference: \n",
    "https://www.bilibili.com/video/BV1Sv4y1A7dz/?spm_id_from=333.788&vd_source=295aeb7cc6407338dd3e15d41a6b90ed\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# generate data\n",
    "# logits shape: [Batchsize, Number of classes]\n",
    "batchsize = 2\n",
    "num_class = 4\n",
    "\n",
    "logits = torch.randn(batchsize, num_class) # the output of a model\n",
    "target_indices = torch.randint(num_class, size=(batchsize,)) # delta-type target distribution\n",
    "target_logits = torch.randn(batchsize, num_class).softmax(dim=1) # the target is a distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c8699d",
   "metadata": {},
   "source": [
    "The following description is taken from en.wikipedia on Cross entropy.\n",
    "## Cross entropy\n",
    "In information theory, the cross-entropy between two probability distributions $p$ and $q$ over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution $q$, rather than the true distribution $p$.\n",
    "### Definition\n",
    "The cross-entropy of the distribution $q$ relative to a distribution $p$ over a given set is defined as follows.\n",
    "$$\n",
    "H(p,q)=-\\mathrm{E}_p[\\log q],\n",
    "$$\n",
    "where $\\mathrm{E}_p$ is the expected value operator w.r.t the distribution $p$.\n",
    "### Discrete case\n",
    "For discrete probability distributions $p$ and $q$ with the same support $\\mathcal{X}$, this means\n",
    "$$\n",
    "H(p,q)=-\\sum_{x\\in\\mathcal{X}}p(x)\\log q(x)\n",
    "$$\n",
    "### Continuous case\n",
    "The situation for continuous distributions is analogous. We have to assume that $p$ and $q$ are absolutely continuous w.r.t. some reference measure $r$ (usually $r$ is a Lebesgue measure on a Borel $\\sigma$-algebra). Let $P$ and $Q$ be probability density functions of $p$ and $q$ w.r.t. $r$. Then\n",
    "$$\n",
    "-\\int_{\\mathcal{X}}P(x)\\log Q(x)\\mathrm{d}r(x)=\\mathrm{E}_{p}[-\\log Q]\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "H(p,q)=-\\int_{\\mathcal{X}}P(x)\\log Q(x)\\mathrm{d}r(x)\n",
    "$$\n",
    "Note that the notation is also used for a different concept, the joint entropy of $p$ and $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a7ca9bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce loss1: 1.7834562063217163\n",
      "ce loss2: 1.9118367433547974\n"
     ]
    }
   ],
   "source": [
    "## 1. Call cross entropy loss\n",
    "\n",
    "### method 1 for CE loss: when targets are integers, like classification tasks\n",
    "ce_loss_fn = nn.CrossEntropyLoss() # instantiation, this func will perform log-exp-sum operations for you\n",
    "ce_loss1 = ce_loss_fn(logits, target_indices) # The input is expected to contain raw, unnormalized scores for each class.\n",
    "\n",
    "# Here the f at the beginning means format. This is a way of printing in Python3.\n",
    "# You can also perform arithmetic operations in the curly brackets.\n",
    "print(f\"ce loss1: {ce_loss1}\")  \n",
    "\n",
    "### method 2 for CE loss: when target is a distribution\n",
    "ce_loss2 = ce_loss_fn(logits, target_logits)\n",
    "print(f\"ce loss2: {ce_loss2}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70238fbf",
   "metadata": {},
   "source": [
    "## Relation to Maximum likelihood\n",
    "The part is taken from wikipedia on Cross entropy.\n",
    "\n",
    "In classifiction problems we want to estimate the probability of different outcomes. Let the estimated probability of outcome $i$ be $q_{\\theta}(X=i)$ with to-be-optimized parameters $\\theta$ and let the frequency (empirical probability) of outcome $i$ in the training set be $p(X=i)$. Given $N$ conditionally independent samples in the training set, then the likelihood of the parameters $\\theta$ of the model on the training set $q_{\\theta}(X=i)$ is \n",
    "$$\n",
    "\\mathcal{L}(\\theta)=\\prod_{i\\in X}(\\text{est. probability of }i)^{\\text{numbers of occurences of }i}=\\prod_{i\\in X}(q_{\\theta}(X=i))^{Np(X=i)}\n",
    "$$\n",
    "so the likelihood divided by $N$ is,\n",
    "$$\n",
    "\\frac{1}{N}\\log(\\mathcal{L}(\\theta))=\\frac{1}{N}\\log\\prod_{i\\in X}(q_{\\theta}(X=i))^{Np(X=i)}=\\sum_{i}p(X=i)\\log q_{\\theta}(X=i)=-H(p,q)\n",
    "$$\n",
    "so that maximizing the log-likelihood w.r.t. the parameters $\\theta$ is the same as minimizing the cross-entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1cf3a100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll loss: 0.7282825708389282\n"
     ]
    }
   ],
   "source": [
    "## 2. call Negative log-likelihood loss\n",
    "m = nn.LogSoftmax(dim=-1)\n",
    "nll_loss_fn = nn.NLLLoss()\n",
    "nll_loss = nll_loss_fn(m(logits),target_indices)\n",
    "print(f\"nll loss: {nll_loss}\") \n",
    "### NLL loss = CE loss when target distribution is a delta distribution, because of math, see wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6943b",
   "metadata": {},
   "source": [
    "## KL divergence\n",
    "### Definition\n",
    "For discrete probability distributions $P$ and $Q$ defined on the same probability space, the relative entropy from $Q$ to $P$ is defined to be\n",
    "$$\n",
    "D_{KL}(P\\|Q)=\\sum_{x\\in\\mathcal{X}}P(x)\\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "In other words, it is the expectation of the logarithmic difference between the probabilities $P$ and $Q$, where the expectation is taken using the probabilities $P$.\n",
    "\n",
    "For distributions of $P$ and $Q$ of a continuous variable, relative entropy is defined to be the integral:\n",
    "$$\n",
    "D_{KL}(P\\|Q)=\\int_{-\\infty}^{+\\infty}p(x)\\log\\frac{p(x)}{q(x)}\\mathrm{d}x\n",
    "$$\n",
    "where $p$ and $q$ denote the probability densities of $P$ and $Q$. Here we abuse the notations of $P,Q$ and $p,q$ in this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dc1d1b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL loss: 0.0361519530415535\n",
      "KL loss: 0.144607812166214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2904: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## 3. KL divergence\n",
    "kl_loss_fn = nn.KLDivLoss()\n",
    "kl_loss = kl_loss_fn(m(logits), target_logits.softmax(dim=-1))\n",
    "print(f\"KL loss: {kl_loss}\") \n",
    "\n",
    "kl_loss_fn_mathematically_correct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "kl_loss = kl_loss_fn_mathematically_correct(m(logits), target_logits.softmax(dim=-1))\n",
    "print(f\"KL loss: {kl_loss}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293b2d7",
   "metadata": {},
   "source": [
    "The definition of cross-entropy may be formulated using KL divergence, $D_{KL}(p\\|q)$, divergence of $p$ from $q$ (a.k.a the relative entropy of $p$ w.r.t. $q$).\n",
    "$$\n",
    "H(p,q)=H(p)+D_{KL}(p\\|q)\n",
    "$$\n",
    "Generally, target info entropy $H(p)$ is a constant, so optimization via CE loss is equivalent to optimization via KL loss. In particular, when target distribution is a delta distribution, $H(p)=0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3aaac7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce loss sample: tensor([1.5688, 1.4553])\n",
      "KL loss sample: tensor([0.1949, 0.0943])\n",
      "target_info_entropy_sample: tensor([1.3739, 1.3610])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## 4. verify CE loss = Info entroy + KL loss\n",
    "ce_loss_sample_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "ce_loss_sample = ce_loss_sample_fn(logits, torch.softmax(target_logits,dim=-1))\n",
    "print(f\"ce loss sample: {ce_loss_sample}\") \n",
    "\n",
    "kl_loss_sample_fn = nn.KLDivLoss(reduction=\"none\")\n",
    "kl_loss_sample = kl_loss_sample_fn(m(logits), target_logits.softmax(dim=-1)).sum(dim=-1)\n",
    "print(f\"KL loss sample: {kl_loss_sample}\") \n",
    "\n",
    "target_info_entropy_sample = torch.distributions.Categorical(probs=target_logits.softmax(dim=-1)).entropy()\n",
    "print(f\"target_info_entropy_sample: {target_info_entropy_sample}\") # when target distribution is a delta distribution, info entroy is 0.\n",
    "\n",
    "# Generally, target info entropy is a constant, so optimization via CE loss is equivalent to optimization via KL loss\n",
    "print(torch.allclose(ce_loss_sample,kl_loss_sample+target_info_entropy_sample)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e85fa14",
   "metadata": {},
   "source": [
    "## Binary cross entropy loss\n",
    "$$\n",
    "l_n(x_n,y_n)=-[y_n\\log p(x_n) + (1-y_n)\\log(1-p(x_n))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f82cae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE_loss: 0.9651271104812622\n",
      "nll_loss binary: 0.9651271104812622\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## 5. Binary cross entropy loss\n",
    "BCE_loss_fn = nn.BCELoss()\n",
    "logits = torch.randn(batchsize)\n",
    "prob_1 = torch.sigmoid(logits)\n",
    "target = torch.randint(2, size=(batchsize, ))\n",
    "BCE_loss = BCE_loss_fn(prob_1, target.float())\n",
    "print(f\"BCE_loss: {BCE_loss}\")\n",
    "\n",
    "### calculate BCE loss using NLL loss\n",
    "prob_1 = prob_1.unsqueeze(dim=-1)\n",
    "prob_0 = 1 - prob_1\n",
    "prob = torch.cat([prob_0, prob_1], dim=-1)\n",
    "nll_loss_binary = nll_loss_fn(torch.log(prob), target)\n",
    "print(f\"nll_loss binary: {nll_loss_binary}\")\n",
    "print(torch.allclose(nll_loss_binary,BCE_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c71496fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_loss: 0.9742875099182129\n"
     ]
    }
   ],
   "source": [
    "## 6. cosine similarity loss\n",
    "cosine_loss_fn = nn.CosineEmbeddingLoss()\n",
    "v1 = torch.randn(batchsize, 512)\n",
    "v2 = torch.randn(batchsize, 512)\n",
    "target = torch.randint(2, size=(batchsize, ))*2 - 1 # target is supposed to be -1 or 1\n",
    "cosine_loss = cosine_loss_fn(v1,v2,target)\n",
    "print(f\"cosine_loss: {cosine_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6135ad",
   "metadata": {},
   "source": [
    "# Summary of PyTorch Loss-Input Confusion (Cheatsheet)\n",
    "https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/other/pytorch-lossfunc-cheatsheet.md\n",
    "\n",
    "- `torch.nn.functional.binary_cross_entropy` takes logistic sigmoid values as inputs\n",
    "- `torch.nn.functional.binary_cross_entropy_with_logits` takes logits as inputs \n",
    "- `torch.nn.functional.cross_entropy` takes logits as inputs (performs `log_softmax` internally)\n",
    "- `torch.nn.functional.nll_loss` is like `cross_entropy` but takes log-probabilities (log-softmax) values as inputs\n",
    "- `torch.nn.functional.cosine_embedding_loss(input1, input2, target)` takes two vectors as inputs, and also a target which is required to be -1 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964bd622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
